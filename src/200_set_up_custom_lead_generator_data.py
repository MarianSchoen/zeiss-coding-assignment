# written by Marian Schoen

# this script automatically generates
# - a scatterplot folder in 'results'. There, one plot per feature (column) 
# of the data set is stored as pdf 
# 

import os
import pandas as pd
import umap
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from plotnine import ggplot, aes, geom_point, theme, element_text, ggtitle, geom_hline, save_as_pdf_pages
import matplotlib.pyplot as plt
from functions import read_clg_data

folder, clg = read_clg_data()

# Check the info of the DataFrame => all but 2 int, or float 
clg.info()
# FakeID is object.
# q_OpeningHours are dtype object, with numerical entries + some char/str.     
# As I don't know what this indicates, I would ask for expert knowledge. 
# For this coding challenge, I interprete it as numeric. 
# Any entry that can not be casted to numeric, will be excluded
opening_hours_manipulated = []
for oh in clg["q_OpeningHours"]: 
    try: 
        the_number = pd.to_numeric(oh)
    except: 
        the_number = "nan"
    opening_hours_manipulated.append(the_number)

nan_indices = opening_hours_manipulated.index("nan")
clg = clg.drop(nan_indices)
del opening_hours_manipulated[-nan_indices]

clg["q_OpeningHours_manipulated"] = opening_hours_manipulated
clg = clg.drop("q_OpeningHours", axis = 1)

# By looking into the data, quickly check what features are in the data set: 
# b_specialisation_* boolean features. As some samples have multiple 
# specialisations, this can not directly be treated as a one-hot-encoded 
# variable.
# q_OpeningDays, q_OpeningHours ? potentially information about a/the store?
# q_2017* Househould and per Capita information, potentially of the city/state the data set comes from
# all features are numeric (or categorically numeric)
# 'b_in_kontakt_gewesen', 'b_gekauft_gesamt' are 0,1 (logical) variables.

# ASSUMPTION: 
# I assume this data set is generated by a product owner, that wants to 
# sell its product to retail outlets. E.g. the product would be some kind 
# medical product, and the outlets would be pharmacies.

# The product owner already contacted 95 pharmacies, 
# and sold its product to 57 of those. 

# The question arising here is: 
# 1. is there a pattern for those pharmacies that rejected/accepted the product. 
# 2. Can we train a ML model to predict which uncontacted pharmacies may buy the product once they are contacted. 

# (Simple) Feature Engineering:
# As described above, I assume that each sample/row 
# of the dataset is one outlet/pharmacy. They are 
# distributed into various location and cities. 
# Even though the city information is not directly in 
# the data, we can derive it: 
city_identifier = [str(clg.loc[i,"q_2017 Total Households"]) + "-" + str(clg.loc[i, "q_2017 Total Population"]) for i in clg.index]
# For the moment, I keep the city identifier out of the dataset


# NOTE: I decide not to scale/normalize any of those features, because
# 1. I use a simple univariate test/visualization per feature and a 
# random forest. 
# THere, each feature is used to split the data independently, therefore 
# no scaling is required. 
# 2. Keeping the data in their natural range will help interpreting the results later on

# select those samples (= rows) that have been visited 
visited_samples = clg[clg["b_in_kontakt_gewesen"] == 1]
visited_samples_sale_label = visited_samples["b_gekauft_gesamt"]

# Explorative Analysis ("pseudo" univariate test): 
# Visualize all features individually, coloured by 
# "b_gekauft_gesamt", in order to detect patterns. Adding dashed mean
# that solve as a univariate test "by eye" help to detect potential 
# discriminator

# initiate an one-column DataFrame, for easier plotnine plotting
ptp = visited_samples[["fakeID"]]
# cast "b_gekauft_gesamt" to string/object, for categorical plotting.
ptp.loc[:,"b_gekauft_gesamt"] = [str(i) for i in visited_samples["b_gekauft_gesamt"]]

path_to_results_scatterplots = folder + "results/scatterplots/"
if not os.path.exists(path_to_results_scatterplots): 
    os.mkdir(path_to_results_scatterplots)

# I generate one plot per column, therefore loop 
for a_column in visited_samples.columns:
    # skip the non numerical columns
    if a_column in ["fakeID", "b_gekauft_gesamt", "b_in_kontakt_gewesen"]: 
        continue 
    
    # remove all non alpha-numerical characters in the column name 
    # (necessary for plotnine + saving with telling file name)
    clean_name = ''.join(l for l in a_column if l.isalnum())

    # make sure the feature is numeric:
    ptp[clean_name] = pd.to_numeric(visited_samples[a_column])

    # calculate the mean per group
    mean_group_0 = ptp.loc[ptp["b_gekauft_gesamt"] == "0", clean_name].mean()
    mean_group_1 = ptp.loc[ptp["b_gekauft_gesamt"] == "1", clean_name].mean()

    # make the plot ...
    the_plot = ggplot(ptp) + aes(x = "reorder(fakeID," + clean_name + ")", y  = clean_name, color = "b_gekauft_gesamt") + geom_point() + ggtitle("Each point is a sample/row in the dataset. Dashed line are mean per group") + geom_hline(yintercept = mean_group_0, colour = "red", linetype = "dashed")  + geom_hline(yintercept = mean_group_1, colour = "blue", linetype = "dashed")

    # ... and store it
    file_name = path_to_results_scatterplots + clean_name + ".pdf"
    the_plot.save(file_name)

    # remove the plotted column from ptp
    ptp = ptp.drop(clean_name, axis = 1)

# plot interpretation: 
# Each plot can be intrepreted as a univariate test, if the dashed mean 
# lines are clearly separated a corresponding test might be 
# significant. 
# (i) -> there is no feature that clearly separates
# the data into "gekauft"/"nicht gekauft". 
# (ii) "b_specialisation_b" is always 1, when "gekauft". However, there are a lot of "b_specialisation_b" are 1 without "gekauft". Similar observation for "b_sepcialisation_d".
# (iii) in average, "q_2017 HHs: 5th Quintile (68.759 and above)" is higher (> 100 000) in the "gekauft" group. This is a 1.5 fold change compared to the "nicht gekauft" group with ~75 000. Similar observation for "q_2017 Pop 15+/Edu: University, Fachhochschule", "q_2017 Total Households" and "q_2017 Total Population". 

# Model Learning 
# The explorative analysis showed that some features in the data set 
# show a (minor) discriminative power. 
# Therefore, I train a random forrest to predict "b_gekauft_gesamt" 
# using all features (besides "b_in_kontakt_gewesen" and "fakeID") 
# on all "b_in_kontakt_gewesen" samples. 
# As there are "only" 95 samples, I check the model performance in a 
# leave-one-out cross-validation. 
# With the final model, I predict which of the > 3000 unvisited 
# samples/pharmacies potentially should be visited.

# For that, drop those columns that are not used as predictors
visited_samples = visited_samples.drop(["b_in_kontakt_gewesen", "b_gekauft_gesamt", "fakeID"], axis = 1)

# In this version, there is only one hyperparameter: 
n_trees = 25 # n_trees is manually tuned. In order to get better result, 
# we could increase/decrease this number and check the models performance

# initiate an empty array to store the predicted classes
predicted_class = []

# I implemented the LOOCV myself, and not use, e.g., a sklearn implementation. 
# So, loop over each index ...
for leave_out_index in visited_samples.index: 
    # initiate a random forest instance
    rf = RandomForestClassifier(
        n_estimators = n_trees # number of trees in the forest 
    )

    # remove leave out index from temporary training data, and label
    tmp_X = visited_samples.drop(index = leave_out_index)
    tmp_y = visited_samples_sale_label.drop(index = leave_out_index)

    # fit the tree
    rf.fit(tmp_X, tmp_y)

    # predict the left out sample    
    leave_out_prediction = rf.predict(visited_samples[visited_samples.index == leave_out_index])[0]
    predicted_class.append(leave_out_prediction)

# LOOCV contingecny table: 
loocv_ct = pd.crosstab(visited_samples_sale_label, predicted_class)
# => accuracy is slightly above 50 %, which for a 2 level prediciton is same as guessing
#NOTE: I decided to use a RF for this challenge. In order to show you all 
# aspects of an explorative analysis, I keep using this RF model, even 
# though its predictions are bad. 
#TODO: check different ML models, whether they predict better (postponed for 
# later versions of this project)

rf = RandomForestClassifier(
    n_estimators = n_trees # number of trees in the forest 
)
rf.fit(visited_samples, visited_samples_sale_label)
clg["RF_prediction"] = rf.predict(all_samples)


# contingency table: 
train_ct = pd.crosstab(visited_samples_sale_label, rf.predict(visited_samples))

# Check feature importances, in order to interpret the ML model
importances = rf.feature_importances_
fi = pd.DataFrame(importances, columns = ["MDI"])
fi["feature"] = [str(i) for i in visited_samples.columns]

fi_plot = ggplot(fi) + aes(x = "reorder(feature, MDI)", y = "MDI") + geom_point() + theme(axis_text_x = element_text(angle = 70, hjust = 1, size = 8)) + ggtitle("Mean Decrease Importancy")
fi_plot.save(folder + "results/RF_MDI_plot.pdf")


# With the model at hand, I predict all samples:
# drop those features that have not been used in the RF
all_samples = clg.drop(["b_in_kontakt_gewesen", "b_gekauft_gesamt", "fakeID"], axis = 1)

# predict those samples
clg["RF_prediction"] = rf.predict(all_samples)


# To this end, we have a set of features that should potentially be cross checked by an expert that indicate whether a new sample/pharmacy should be visited as the change of a sale is higher. 
# Additionally, we did a similar prediction based on a random forest model. 
# In a last step, we're going to combine those 2 steps. I embedd 
# the data into 2 dimension using the UMAP approach. In the resulting 
# plot, I highlight both the potentially beneficial features, and the 
# prediction of the RF model.

# As UMAP estimates neighborhoods/distances in the high
# dimensional space, we need to scale all data. 
scaled_data = StandardScaler().fit_transform(clg.drop(["b_in_kontakt_gewesen", "b_gekauft_gesamt", "fakeID", "RF_prediction"], axis = 1))

reducer = umap.UMAP()

clg_in_2D = reducer.fit_transform(scaled_data)

clg["UMAP1"] = clg_in_2D[:,0]
clg["UMAP2"] = clg_in_2D[:,1]

clg["b_gekauft_gesamt"] = [str(i) for i in clg["b_gekauft_gesamt"]]

umap_plot_default = ggplot(clg) + aes(x = "UMAP1", y = "UMAP2") + ggtitle("Each dot is a sample/row/pharmacy")


plots = []
plots.append(umap_plot_default + geom_point())
# As only 95 points are blue, and all others are red, I use the 
# alpha value to highlight the blue points
plots.append(
    umap_plot_default + aes(colour = "b_gekauft_gesamt", alpha = "b_gekauft_gesamt") + geom_point()
)

for a_column in clg.columns:
    if a_column in ["fakeID", "b_gekauft_gesamt", "b_in_kontakt_gewesen", "UMAP1", "UMAP2"]: 
        continue 
    
    plots.append(umap_plot_default + aes(colour = a_column, alpha = "b_gekauft_gesamt", shape = "b_gekauft_gesamt") + geom_point())

#TODO: that command does not transport the colouring. 
save_as_pdf_pages(plots, folder + "results/ctg_in_UMAP.pdf")
